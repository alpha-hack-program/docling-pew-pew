{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "207cabea-3628-47f4-b8c7-6aee270b86aa",
   "metadata": {},
   "source": [
    "# Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cd17b00-6a55-49ad-a8bc-190988143593",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "pip install requests docling transformers ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a795ccba-4313-40b9-ab56-aa89f286629b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "pip install llama-index-core llama-index-readers-docling llama-index-node-parser-docling llama-index-embeddings-huggingface llama-index-llms-huggingface-api llama-index-vector-stores-milvus llama-index-readers-file python-dotenv;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff599941-1a81-4f9d-8e83-f9c53f443039",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Docling and chunker for text parsing\n",
    "from docling.document_converter import DocumentConverter\n",
    "from docling.chunking import HybridChunker\n",
    "## embeddings generator\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e3ac39-ed1c-46f0-b5d1-6c38431e0b86",
   "metadata": {},
   "source": [
    "# Docling ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516035be-907d-4659-9fe8-dcb1bc5c657f",
   "metadata": {},
   "source": [
    "## Define global variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27a0587e-56e9-44cf-bcfa-64f167e0713b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Source for the documents for docling parsing of chunks\n",
    "source_file = \"http://minio-ui-ic-shared-minio.apps.ocp.sandbox2941.opentlc.com/api/v1/download-shared-object/aHR0cDovLzEyNy4wLjAuMTo5MDAwL3BkZnMvUmVkX0hhdF9PcGVuU2hpZnRfQUlfU2VsZi1NYW5hZ2VkLTIuMTYtR2V0dGluZ19zdGFydGVkX3dpdGhfUmVkX0hhdF9PcGVuU2hpZnRfQUlfU2VsZi1NYW5hZ2VkLWVuLVVTLnBkZj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUVNWURIR0pFNDBQQzczVElMWVBFJTJGMjAyNTAxMTQlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMTE0VDE0MTIxNFomWC1BbXotRXhwaXJlcz00MzIwMCZYLUFtei1TZWN1cml0eS1Ub2tlbj1leUpoYkdjaU9pSklVelV4TWlJc0luUjVjQ0k2SWtwWFZDSjkuZXlKaFkyTmxjM05MWlhraU9pSkZUVmxFU0VkS1JUUXdVRU0zTTFSSlRGbFFSU0lzSW1WNGNDSTZNVGN6Tmprd056QTNPQ3dpY0dGeVpXNTBJam9pYldsdWFXOGlmUS53c2NjZ3dCSVdoYl92UFBYMmlyZUZxVHh4MzZyN18xX3JKcXZLUUcxWVhJXzh1TXBKQUxzb0xkRWU3VXBROE5lWHFtdzNRX1ppU1duUDdXSzJ0SlFYdyZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmdmVyc2lvbklkPW51bGwmWC1BbXotU2lnbmF0dXJlPWY1MDBmNDc4MThhYzE3YjU4NmQ5YmU4ZTkyMzI3OTQ0ZDlhNmQ5YWFiNDU1ZjI4ZmQzM2I3ODVmYTc0NjliZDA\"  # document per local path or URL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6664e81-e172-4255-b296-3f4ae8b05442",
   "metadata": {},
   "source": [
    "## 0) Ingest documentation - Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9131a61-d282-4a7f-9727-fb46a6d29479",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!-- image -->\n",
      "\n",
      "## Red Hat OpenShift AI Self-Managed 2.16\n",
      "\n",
      "## Getting started with Red Hat OpenShift AI Self-Managed\n",
      "\n",
      "Learn how to work in an OpenShift AI environment\n",
      "\n",
      "## Red Hat OpenShift AI Self-Managed 2.16 Getting started with Red Hat OpenShift AI Self-Managed\n",
      "\n",
      "Learn how to work in an OpenShift AI environment\n",
      "\n",
      "## Legal Notice\n",
      "\n",
      "Copyright © 2024 Red Hat, Inc.\n",
      "\n",
      "The text of and illustrations in this document are licensed by Red Hat under a Creative Commons Attribution-Share Alike 3.0 Unported license (\"CC-BY-SA\"). An explanation of CC-BY-SA is available at\n",
      "\n",
      "http://creativecommons.org/licenses/by-sa/3.0/\n",
      "\n",
      "- . In accordance with CC-BY-SA, if you distribute this document or an adaptation of it, you must provide the URL for the original version.\n",
      "\n",
      "Red Hat, as the licensor of this document, waives the right to enforce, and agrees not to assert, Section 4d of CC-BY-SA to the fullest extent permitted by applicable law.\n",
      "\n",
      "Red Hat, Red Hat Enterprise Linux, the Shadowman logo, the Red Hat logo, JBoss, OpenShift, Fedora, the Infinity logo, and RHCE are trademarks of Red Hat, Inc., registered in the United States and other countries.\n",
      "\n",
      "Linux ® is the registered trademark of Linus Torvalds in the United States and other countries.\n",
      "\n",
      "Java ® is a registered trademark of Oracle and/or its affiliates.\n",
      "\n",
      "XFS ® is a trademark of Silicon Graphics International Corp. or its subsidiaries in the United States and/or other countries.\n",
      "\n",
      "MySQL ® is a registered trademark of MySQL AB in the United States, the European Union and other countries.\n",
      "\n",
      "Node.js ® is an official trademark of Joyent. Red Hat is not formally related to or endorsed by the official Joyent Node.js open source or commercial project.\n",
      "\n",
      "The OpenStack ® Word Mark and OpenStack logo are either registered trademarks/service marks or trademarks/service marks of the OpenStack Foundation, in the United States and other countries and are used with the OpenStack Foundation's permission. We are not affiliated with, endorsed or sponsored by the OpenStack Foundation, or the OpenStack community.\n",
      "\n",
      "All other trademarks are the property of their respective owners.\n",
      "\n",
      "## Abstract\n",
      "\n",
      "Learn how to work in an OpenShift AI environment.\n",
      "\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "\n",
      "Table of Contents\n",
      "\n",
      "| CHAPTER 1. OVERVIEW                                                                                                                                                                                                                                                                                     |   3 |\n",
      "|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----|\n",
      "| 1.1. DATA SCIENCE WORKFLOW                                                                                                                                                                                                                                                                              |   3 |\n",
      "| 1.2. ABOUT THIS GUIDE                                                                                                                                                                                                                                                                                   |   4 |\n",
      "| . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  CHAPTER 2. LOGGING IN TO OPENSHIFT AI                |   5 |\n",
      "| . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  CHAPTER 3. CREATING A DATA SCIENCE PROJECT           |   6 |\n",
      "| . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  CHAPTER 4. CREATING A WORKBENCH AND SELECTING AN IDE |   8 |\n",
      "| 4.1. ABOUT WORKBENCH IMAGES                                                                                                                                                                                                                                                                             |   8 |\n",
      "| 4.2. BUILDING THE RSTUDIO SERVER WORKBENCH IMAGES                                                                                                                                                                                                                                                       |  11 |\n",
      "| 4.3. CREATING A WORKBENCH                                                                                                                                                                                                                                                                               |  13 |\n",
      "| . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  CHAPTER 5. NEXT STEPS                                |  17 |\n",
      "| 5.1. ADDITIONAL RESOURCES                                                                                                                                                                                                                                                                               |  18 |\n",
      "\n",
      "## CHAPTER 1. OVERVIEW\n",
      "\n",
      "Red Hat OpenShift AI is an artificial intelligence (AI) platform that provides tools to rapidly train, serve, and monitor machine learning (ML) models onsite, in the public cloud, or at the edge.\n",
      "\n",
      "OpenShift AI provides a powerful AI/ML platform for building AI-enabled applications. Data scientists and MLOps engineers can collaborate to move from experiment to production in a consistent environment quickly.\n",
      "\n",
      "You can deploy OpenShift AI on any supported version of OpenShift, whether on-premise, in the cloud, or in disconnected environments. For details on supported versions, see Red Hat OpenShift AI: Supported Configurations.\n",
      "\n",
      "## 1.1. DATA SCIENCE WORKFLOW\n",
      "\n",
      "For the purpose of getting you started with OpenShift AI, Figure 1 illustrates a simplified data science workflow. The real world process of developing ML models is an iterative one.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "The simplified data science workflow for predictive AI use cases includes the following tasks:\n",
      "\n",
      "- Defining your business problem and setting goals to solve it.\n",
      "- Gathering, cleaning, and preparing data. Data often has to be federated from a range of sources, and exploring and understanding data plays a key role in the success of a data science project.\n",
      "- Evaluating and selecting ML models for your business use case.\n",
      "- Train models for your business use case by tuning model parameters based on your set of training data. In practice, data scientists train a range of models, and compare performance while considering tradeoffs such as time and memory constraints.\n",
      "- Integrate models into an application, including deployment and testing. After model training, the next step of the workflow is production. Data scientists are often responsible for putting the model in production and making it accessible so that a developer can integrate the model into an application.\n",
      "\n",
      "Monitor and manage deployed models. Depending on the organization, data scientists, data\n",
      "\n",
      "- Monitor and manage deployed models. Depending on the organization, data scientists, data engineers, or ML engineers must monitor the performance of models in production, tracking prediction and performance metrics.\n",
      "- Refine and retrain models. Data scientists can evaluate model performance results and refine models to improve outcome by excluding or including features, changing the training data, and modifying other configuration parameters.\n",
      "\n",
      "## 1.2. ABOUT THIS GUIDE\n",
      "\n",
      "This guide assumes you are familiar with data science and ML Ops concepts. It describes the following tasks to get you started with using OpenShift AI:\n",
      "\n",
      "- Log in to the OpenShift AI dashboard\n",
      "- Create a data science project\n",
      "- If you have data stored in Object Storage, configure a connection to more easily access it\n",
      "- Create a workbench and choose an IDE, such as JupyterLab or code-server, for your data scientist development work\n",
      "- Learn where to get information about the next steps:\n",
      "- Developing and training a model\n",
      "- Automating the workflow with pipelines\n",
      "- Implementing distributed workloads\n",
      "- Testing your model\n",
      "- Deploying your model\n",
      "- Monitoring and managing your model\n",
      "\n",
      "See also OpenShift AI tutorial: Fraud detection example . It provides step-by-step guidance for using OpenShift AI to develop and train an example model in JupyterLab, deploy the model, and refine the model by using automated pipelines.\n",
      "\n",
      "## CHAPTER 2. LOGGING IN TO OPENSHIFT AI\n",
      "\n",
      "After you install OpenShift AI, log in to the OpenShift AI dashboard so that you can set up your development and deployment environment.\n",
      "\n",
      "## Prerequisites\n",
      "\n",
      "- You know the OpenShift AI identity provider and your login credentials.\n",
      "- If you are a data scientist, data engineer, or ML engineer, your administrator must provide you with the OpenShift AI instance URL, for example:\n",
      "\n",
      "https://rhoai-dashboard-redhat-oaiapplications.apps.example.abc1.p1.openshiftapps.com/\n",
      "\n",
      "- You have the latest version of one of the following supported browsers:\n",
      "- Google Chrome\n",
      "- Mozilla Firefox\n",
      "- Safari\n",
      "\n",
      "## Procedure\n",
      "\n",
      "- 1. Browse to the OpenShift AI instance URL and click Log in with OpenShift .\n",
      "- If you have access to OpenShift, you can browse to the OpenShift web console and click\n",
      "\n",
      "the Application Launcher (\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "→ Red Hat OpenShift AI .\n",
      "\n",
      "- 2.  Click the name of your identity provider, for example, GitHub , Google , or your company's single sign-on method.\n",
      "- 3.  Enter your credentials and click Log in (or equivalent for your identity provider).\n",
      "\n",
      "## Verification\n",
      "\n",
      "- The OpenShift AI dashboard opens on the Home page.\n",
      "\n",
      "## CHAPTER 3. CREATING A DATA SCIENCE PROJECT\n",
      "\n",
      "To implement a data science workflow, you must create a project. In OpenShift, a project is a Kubernetes namespace with additional annotations, and is the main way that you can manage user access to resources. A project organizes your data science work in one place and also allows you to collaborate with other developers and data scientists in your organization.\n",
      "\n",
      "Within a project, you can add the following functionality:\n",
      "\n",
      "- Connections so that you can access data without having to hardcode information like endpoints or credentials.\n",
      "- Workbenches for working with and processing data, and for developing models.\n",
      "- Deployed models so that you can test them and then integrate them into intelligent applications. Deploying a model makes it available as a service that you can access by using an API.\n",
      "- Pipelines for automating your ML workflow.\n",
      "\n",
      "## Prerequisites\n",
      "\n",
      "- You have logged in to Red Hat OpenShift AI.\n",
      "- If you are using OpenShift AI groups, you are part of the user group or admin group (for example, rhoai-users or rhoai-admins ) in OpenShift.\n",
      "- You have the appropriate roles and permissions to create projects.\n",
      "\n",
      "## Procedure\n",
      "\n",
      "- 1. From the OpenShift AI dashboard, select Data Science Projects . The Data Science Projects page shows a list of projects that you can access. For each userrequested project in the list, the Name column shows the project display name, the user who requested the project, and the project description.\n",
      "- 2.  Click Create project .\n",
      "- 3.  In the Create project dialog, update the Name field to enter a unique display name for your project.\n",
      "- 4.  Optional: If you want to change the default resource name for your project, click Edit resource name .\n",
      "- The resource name is what your resource is labeled in OpenShift. Valid characters include lowercase letters, numbers, and hyphens (-). The resource name cannot exceed 30 characters, and it must start with a letter and end with a letter or number.\n",
      "\n",
      "Note: You cannot change the resource name after the project is created. You can edit only the display name and the description.\n",
      "\n",
      "- 5.  Optional: In the Description field, provide a project description.\n",
      "- 6.  Click Create .\n",
      "\n",
      "## Verification\n",
      "\n",
      "- A project details page opens. From this page, you can add connections, create workbenches, configure pipelines, and deploy models.\n",
      "\n",
      "## CHAPTER 4. CREATING A WORKBENCH AND SELECTING AN IDE\n",
      "\n",
      "A workbench is an isolated area where you can examine and work with ML models. You can also work with data and run programs, for example to prepare and clean data. While a workbench is not required if, for example, you only want to service an existing model, one is needed for most data science workflow tasks, such as writing code to process data or training a model.\n",
      "\n",
      "When you create a workbench, you specify an image (an IDE, packages, and other dependencies). Supported IDEs include JupyterLab, code-server, and RStudio (Technology Preview).\n",
      "\n",
      "The IDEs are based on a server-client architecture. Each IDE provides a server that runs in a container on the OpenShift cluster, while the user interface (the client) is displayed in your web browser. For example, the Jupyter notebook server runs in a container on the Red Hat OpenShift cluster. The client is the JupyterLab interface that opens in your web browser on your local computer. All of the commands that you enter in JupyterLab are executed by the notebook server. Similarly, other IDEs like code-server or RStudio Server provide a server that runs in a container on the OpenShift cluster, while the user interface is displayed in your web browser. This architecture allows you to interact through your local computer in a browser environment, while all processing occurs on the cluster. The cluster provides the benefits of larger available resources and security because the data being processed never leaves the cluster.\n",
      "\n",
      "In a workbench, you can also configure connections (to access external data for training models and to save models so that you can deploy them) and cluster storage (for persisting data). Workbenches within the same project can share models and data through object storage with the data science pipelines and model servers.\n",
      "\n",
      "For data science projects that require data retention, you can add container storage to the workbench you are creating.\n",
      "\n",
      "Within a project, you can create multiple workbenches. When to create a new workbench depends on considerations, such as the following:\n",
      "\n",
      "- The workbench configuration (for example, CPU, RAM, or IDE). If you want to avoid editing the configuration of an existing workbench's configuration to accommodate a new task, you can create a new workbench instead.\n",
      "- Separation of tasks or activities. For example, you might want to use one workbench for your Large Language Models (LLM) experimentation activities, another workbench dedicated to a demo, and another workbench for testing.\n",
      "\n",
      "## 4.1. ABOUT WORKBENCH IMAGES\n",
      "\n",
      "A workbench image (sometimes referred to as a notebook image) is optimized with the tools and libraries that you need for model development. You can use the provided workbench images or an OpenShift AI administrator can create custom workbench images adapted to your needs.\n",
      "\n",
      "To provide a consistent, stable platform for your model development, many provided workbench images contain the same version of Python. Most workbench images available on OpenShift AI are pre-built and ready for you to use immediately after OpenShift AI is installed or upgraded.\n",
      "\n",
      "For information about Red Hat support of workbench images and packages, see Red Hat OpenShift AI: Supported Configurations.\n",
      "\n",
      "The following table lists the workbench images that are installed with Red Hat OpenShift AI by default.\n",
      "\n",
      "If the preinstalled packages that are provided in these images are not sufficient for your use case, you have the following options:\n",
      "\n",
      "- Install additional libraries after launching a default image. This option is good if you want to add libraries on an ad hoc basis as you develop models. However, it can be challenging to manage the dependencies of installed libraries and your changes are not saved when the workbench restarts.\n",
      "- Create a custom image that includes the additional libraries or packages. For more information, see Creating custom workbench images.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## IMPORTANT\n",
      "\n",
      "Workbench images denoted with (Technology Preview) in this table are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using Technology Preview features in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process. For more information about the support scope of Red Hat Technology Preview features, see Technology Preview Features Support Scope .\n",
      "\n",
      "Table 4.1. Default workbench images\n",
      "\n",
      "| Image name            | Description                                                                                                                                                                                                                                                                                                                  |\n",
      "|-----------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| CUDA                  | If you are working with compute-intensive data science models that require GPU support, use the Compute Unified Device Architecture (CUDA) workbench image to gain access to the NVIDIA CUDA Toolkit. Using this toolkit, you can optimize your work by using GPU- accelerated libraries and optimization tools.             |\n",
      "| Standard Data Science | Use the Standard Data Science workbench image for models that do not require TensorFlow or PyTorch. This image contains commonly-used libraries to assist you in developing your machine learning models.                                                                                                                    |\n",
      "| TensorFlow            | TensorFlow is an open source platform for machine learning. With TensorFlow, you can build, train and deploy your machine learning models. TensorFlow contains advanced data visualization features, such as computational graph visualizations. It also allows you to easily monitor and track the progress of your models. |\n",
      "| PyTorch               | PyTorch is an open source machine learning library optimized for deep learning. If you are working with computer vision or natural language processing models, use the Pytorch workbench image.                                                                                                                              |\n",
      "| Minimal Python        | If you do not require advanced machine learning features, or additional resources for compute-intensive data science work, you can use the Minimal Python image to develop your models.                                                                                                                                      |\n",
      "| TrustyAI              | Use the TrustyAI workbench image to leverage your data science work with model explainability, tracing, and accountability, and runtime monitoring. See the TrustyAI Explainability repository for some example Jupyter notebooks.                                                                                           |\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "| Image name                          | Description                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n",
      "|-------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| code-server                         | With the code-server workbench image, you can customize your workbench environment to meet your needs using a variety of extensions to add new languages, themes, debuggers, and connect to additional services. Enhance the efficiency of your data science work with syntax highlighting, auto-indentation, and bracket matching, as well as an automatic task runner for seamless automation. For more information, see code-server in GitHub. |\n",
      "|                                     | NOTE: Elyra-based pipelines are not available with the code-server workbench image.                                                                                                                                                                                                                                                                                                                                                               |\n",
      "| RStudio Server (Technology preview) | Use the RStudio Server workbench image to access the RStudio IDE, an integrated development environment for R, a programming language for statistical computing and graphics. For more information, see the RStudio Server site.                                                                                                                                                                                                                  |\n",
      "|                                     | To use the  RStudio Server  workbench image, you must first build it by creating a secret and triggering the BuildConfig, and then enable it in the OpenShift AI UI by editing the  rstudio-rhel9  image stream. For more information, see Building the RStudio Server workbench images.                                                                                                                                                          |\n",
      "|                                     | IMPORTANT Disclaimer: Red Hat supports managing workbenches in OpenShift AI. However, Red Hat does not provide support for the RStudio software. RStudio Server is available through https://rstudio.org/ and is subject to RStudio licensing terms. Review the licensing terms before you use this sample workbench.                                                                                                                             |\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "| Image name                                 | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n",
      "|--------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| CUDA - RStudio Server (Technology Preview) | Use the CUDA - RStudio Server workbench image to access the RStudio IDE and NVIDIA CUDA Toolkit. RStudio is an integrated development environment for R, a programming language for statistical computing and graphics. With the NVIDIA CUDA toolkit, you can optimize your work using GPU-accelerated libraries and optimization tools. For more information, see the RStudio Server site.                                                                                                                            |\n",
      "|                                            | To use the  CUDA - RStudio Server  workbench image, you must first build it by creating a secret and triggering the BuildConfig, and then enable it in the OpenShift AI UI by editing the  cuda-rstudio-rhel9  image stream. For more information, see Building the RStudio Server workbench images.                                                                                                                                                                                                                   |\n",
      "|                                            | IMPORTANT Disclaimer: Red Hat supports managing workbenches in OpenShift AI. However, Red Hat does not provide support for the RStudio software. RStudio Server is available through https://rstudio.org/ and is subject to RStudio licensing terms. Review the licensing terms before you use this sample workbench. The  CUDA - RStudio Server  workbench image contains NVIDIA CUDA technology. CUDA licensing information is available at https://docs.nvidia.com/cuda/. Review the licensing terms before you use |\n",
      "\n",
      "## 4.2. BUILDING THE RSTUDIO SERVER WORKBENCH IMAGES\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## IMPORTANT\n",
      "\n",
      "The RStudio Server and CUDA - RStudio Server workbench images are currently available in Red Hat OpenShift AI as Technology Preview features.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## NOTE\n",
      "\n",
      "The RStudio Server workbench images are currently unavailable for disconnected environments.\n",
      "\n",
      "Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.\n",
      "\n",
      "For more information about the support scope of Red Hat Technology Preview features, see Technology Preview Features Support Scope .\n",
      "\n",
      "Red Hat OpenShift AI includes the following RStudio Server workbench images:\n",
      "\n",
      "- RStudio Server workbench image\n",
      "\n",
      "With the\n",
      "\n",
      "RStudio Server\n",
      "\n",
      "workbench image, you can access the RStudio IDE, an integrated\n",
      "\n",
      "With the RStudio Server workbench image, you can access the RStudio IDE, an integrated development environment for the R programming language. R is used for statistical computing and graphics to support data analysis and predictions.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## IMPORTANT\n",
      "\n",
      "Disclaimer: Red Hat supports managing workbenches in OpenShift AI. However, Red Hat does not provide support for the RStudio software. RStudio Server is available through rstudio.org and is subject to their licensing terms. You should review their licensing terms before you use this sample workbench.\n",
      "\n",
      "## CUDA - RStudio Server workbench image\n",
      "\n",
      "- With the CUDA - RStudio Server workbench image, you can access the RStudio IDE and\n",
      "\n",
      "NVIDIA CUDA Toolkit. The RStudio IDE is an integrated development environment for the R programming language for statistical computing and graphics. With the NVIDIA CUDA toolkit, you can enhance your work by using GPU-accelerated libraries and optimization tools.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## IMPORTANT\n",
      "\n",
      "Disclaimer: Red Hat supports managing workbenches in OpenShift AI. However, Red Hat does not provide support for the RStudio software. RStudio Server is available through rstudio.org and is subject to their licensing terms. You should review their licensing terms before you use this sample workbench.\n",
      "\n",
      "The CUDA - RStudio Server workbench image contains NVIDIA CUDA technology. CUDA licensing information is available in the CUDA Toolkit documentation. You should review their licensing terms before you use this sample workbench.\n",
      "\n",
      "To use the RStudio Server and CUDA - RStudio Server workbench images, you must first build them by creating a secret and triggering the BuildConfig , and then enable them in the OpenShift AI UI by editing the rstudio-rhel9 and cuda-rstudio-rhel9 image streams.\n",
      "\n",
      "## Prerequisites\n",
      "\n",
      "- Before starting the RStudio Server build process, you have at least 1 CPU and 2Gi memory available for rstudio-server-rhel9 , and 1.5 CPUs and 8Gi memory available for cuda-rstudioserver-rhel9 on your cluster.\n",
      "- You are logged in to your OpenShift cluster.\n",
      "- You have the cluster-admin role in OpenShift.\n",
      "- You have an active Red Hat Enterprise Linux (RHEL) subscription.\n",
      "\n",
      "## Procedure\n",
      "\n",
      "- 1. Create a secret with Subscription Manager credentials. These are usually your Red Hat\n",
      "- Customer Portal username and password. Note: The secret must be named rhel-subscription-secret , and its USERNAME and PASSWORD keys must be in capital letters.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "oc create secret generic rhel-subscription-secret --from-literal=USERNAME=<username> -from-literal=PASSWORD=<password> -n redhat-ods-applications\n",
      "\n",
      "- 2.  Start the build:\n",
      "- a.  To start the lightweight RStudio Server build:\n",
      "\n",
      "oc start-build rstudio-server-rhel9 -n redhat-ods-applications --follow\n",
      "\n",
      "- b.  To start the CUDA-enabled RStudio Server build, trigger the cuda-rhel9 BuildConfig:\n",
      "\n",
      "oc start-build cuda-rhel9 -n redhat-ods-applications --follow\n",
      "\n",
      "The cuda-rhel9 build is a prerequisite for cuda-rstudio-rhel9. The cuda-rstudio-rhel9 build starts automatically.\n",
      "\n",
      "- 3.  Confirm that the build process has completed successfully using the following command. Successful builds appear as Complete .\n",
      "\n",
      "oc get builds -n redhat-ods-applications\n",
      "\n",
      "- 4.  After the builds complete successfully, use the following commands to make the workbench images available in the OpenShift AI UI.\n",
      "- a.  To enable the RStudio Server workbench image:\n",
      "\n",
      "oc label -n redhat-ods-applications imagestream rstudio-rhel9 opendatahub.io/notebookimage='true'\n",
      "\n",
      "- b.  To enable the CUDA - RStudio Server workbench image:\n",
      "\n",
      "oc label -n redhat-ods-applications imagestream cuda-rstudio-rhel9 opendatahub.io/notebook-image='true'\n",
      "\n",
      "## Verification\n",
      "\n",
      "- You can see RStudio Server and CUDA - RStudio Server images on the Applications → Enabled menu in the Red Hat OpenShift AI dashboard.\n",
      "- You can see R Studio Server or CUDA - RStudio Server in the Data Science Projects → Workbenches → Create workbench → Notebook image → Image selection dropdown list.\n",
      "\n",
      "## 4.3. CREATING A WORKBENCH\n",
      "\n",
      "When you create a workbench, you specify an image (an IDE, packages, and other dependencies). You can also configure connections, cluster storage, and add container storage.\n",
      "\n",
      "## Prerequisites\n",
      "\n",
      "- You have logged in to Red Hat OpenShift AI.\n",
      "- If you use OpenShift AI groups, you are part of the user group or admin group (for example, rhoai-users or rhoai-admins ) in OpenShift.\n",
      "- You created a project.\n",
      "\n",
      "If you created a Simple Storage Service (S3) account outside of Red Hat OpenShift AI and you\n",
      "\n",
      "- If you created a Simple Storage Service (S3) account outside of Red Hat OpenShift AI and you want to create connections to your existing S3 storage buckets, you have the following credential information for the storage buckets:\n",
      "- Endpoint URL\n",
      "- Access key\n",
      "- Secret key\n",
      "- Region\n",
      "- Bucket name\n",
      "\n",
      "For more information, see Working with data in an S3-compatible object store .\n",
      "\n",
      "## Procedure\n",
      "\n",
      "- 1. From the OpenShift AI dashboard, click Data Science Projects . The Data Science Projects page opens.\n",
      "- 2.  Click the name of the project that you want to add the workbench to. A project details page opens.\n",
      "- 3.  Click the Workbenches tab.\n",
      "- 4.  Click Create workbench . The Create workbench page opens.\n",
      "- 5.  In the Name field, enter a unique name for your workbench.\n",
      "- 6.  Optional: If you want to change the default resource name for your workbench, click Edit resource name .\n",
      "- The resource name is what your resource is labeled in OpenShift. Valid characters include lowercase letters, numbers, and hyphens (-). The resource name cannot exceed 30 characters, and it must start with a letter and end with a letter or number.\n",
      "\n",
      "Note: You cannot change the resource name after the workbench is created. You can edit only the display name and the description.\n",
      "\n",
      "- 7.  Optional: In the Description field, enter a description for your workbench.\n",
      "- 8.  In the Notebook image section, complete the fields to specify the workbench image to use with your workbench.\n",
      "- From the Image selection list, select a workbench image that suits your use case. A workbench image includes an IDE and Python packages (reusable code). Optionally, click View package information to view a list of packages that are included in the image that you selected.\n",
      "\n",
      "If the workbench image has multiple versions available, select the workbench image version to use from the Version selection list. To use the latest package versions, Red Hat recommends that you use the most recently added image.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "9.  In the\n",
      "\n",
      "Deployment size\n",
      "\n",
      "section, from the\n",
      "\n",
      "Container size\n",
      "\n",
      "list, select a container size for your\n",
      "\n",
      "## NOTE\n",
      "\n",
      "You can change the workbench image after you create the workbench.\n",
      "\n",
      "- 9.  In the Deployment size section, from the Container size list, select a container size for your server. The container size specifies the number of CPUs and the amount of memory allocated to the container, setting the guaranteed minimum (request) and maximum (limit) for both.\n",
      "- 10.  Optional: In the Environment variables section, select and specify values for any environment variables.\n",
      "- Setting environment variables during the workbench configuration helps you save time later because you do not need to define them in the body of your notebooks, or with the IDE command line interface.\n",
      "\n",
      "If you are using S3-compatible storage, add these recommended environment variables:\n",
      "\n",
      "- AWS\\_ACCESS\\_KEY\\_ID specifies your Access Key ID for Amazon Web Services.\n",
      "- AWS\\_SECRET\\_ACCESS\\_KEY specifies your Secret access key for the account specified in AWS\\_ACCESS\\_KEY\\_ID .\n",
      "\n",
      "OpenShift AI stores the credentials as Kubernetes secrets in a protected namespace if you select Secret when you add the variable.\n",
      "\n",
      "- 11.  In the Cluster storage section, configure the storage for your workbench. Select one of the following options:\n",
      "- Create new persistent storage to create storage that is retained after you shut down your workbench. Complete the relevant fields to define the storage:\n",
      "- a.  Enter a name for the cluster storage.\n",
      "- b.  Enter a description for the cluster storage.\n",
      "- c.  Select a storage class for the cluster storage.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## NOTE\n",
      "\n",
      "You cannot change the storage class after you add the cluster storage to the workbench.\n",
      "\n",
      "- d.  Under Persistent storage size , enter a new size in gibibytes or mebibytes.\n",
      "- Use existing persistent storage to reuse existing storage and select the storage from the Persistent storage list.\n",
      "- 12.  Optional: You can add a connection to your workbench. A connection is a resource that contains the configuration parameters needed to connect to a data source or sink, such as an object storage bucket. You can use storage buckets for storing data, models, and pipeline artifacts. You can also use a connection to specify the location of a model that you want to deploy. In the Connections section, use an existing connection or create a new connection:\n",
      "- Use an existing connection as follows:\n",
      "- a.  Click Attach existing connections .\n",
      "- b.  From the Connection list, select a connection that you previously defined.\n",
      "- Create a new connection as follows:\n",
      "- a.  Click Create connection . The Add connection dialog appears.\n",
      "\n",
      "- b.  From the Connection type drop-down list, select the type of connection. The Connection details section appears.\n",
      "- c.  If you selected S3 compatible object storage in the preceding step, configure the connection details:\n",
      "- i. In the Connection name field, enter a unique name for the connection.\n",
      "- ii. Optional: In the Description field, enter a description for the connection.\n",
      "- iii. In the Access key field, enter the access key ID for the S3-compatible object storage provider.\n",
      "- iv.  In the Secret key field, enter the secret access key for the S3-compatible object storage account that you specified.\n",
      "- v.  In the Endpoint field, enter the endpoint of your S3-compatible object storage bucket.\n",
      "- vi.  In the Region field, enter the default region of your S3-compatible object storage account.\n",
      "- vii.  In the Bucket field, enter the name of your S3-compatible object storage bucket.\n",
      "- viii.  Click Create .\n",
      "- d.  If you selected URI in the preceding step, configure the connection details:\n",
      "- i. In the Connection name field, enter a unique name for the connection.\n",
      "- ii. Optional: In the Description field, enter a description for the connection.\n",
      "- iii. In the URI field, enter the Uniform Resource Identifier (URI).\n",
      "- iv.  Click Create .\n",
      "\n",
      "## 13.  Click Create workbench .\n",
      "\n",
      "## Verification\n",
      "\n",
      "- The workbench that you created appears on the Workbenches tab for the project.\n",
      "- Any cluster storage that you associated with the workbench during the creation process appears on the Cluster storage tab for the project.\n",
      "- The Status column on the Workbenches tab displays a status of Starting when the workbench server is starting, and Running when the workbench has successfully started.\n",
      "- Optional: Click the Open link to open the IDE in a new window.\n",
      "\n",
      "## CHAPTER 5. NEXT STEPS\n",
      "\n",
      "The following product documentation provides more information on how to develop, test, and deploy data science solutions with OpenShift AI.\n",
      "\n",
      "## Try the end-to-end tutorial\n",
      "\n",
      "## OpenShift AI tutorial - Fraud detection example\n",
      "\n",
      "Step-by-step guidance to complete the following tasks with an example fraud detection model:\n",
      "\n",
      "- Explore a pre-trained fraud detection model by using a Jupyter notebook.\n",
      "- Deploy the model by using OpenShift AI model serving.\n",
      "- Refine and train the model by using automated pipelines.\n",
      "\n",
      "## Develop and train a model in your workbench IDE\n",
      "\n",
      "## Working in your data science IDE\n",
      "\n",
      "Learn how to access your workbench IDE (JupyterLab, code-server, or RStudio Server).\n",
      "\n",
      "For the JupyterLab IDE, learn about the following tasks:\n",
      "\n",
      "- Creating and importing notebooks\n",
      "- Using Git to collaborate on notebooks\n",
      "- Viewing and installing Python packages\n",
      "- Troubleshooting common problems\n",
      "\n",
      "## Automate your ML workflow with pipelines\n",
      "\n",
      "## Working with data science pipelines\n",
      "\n",
      "Enhance your data science projects on OpenShift AI by building portable machine learning (ML) workflows with data science pipelines, by using Docker containers. Use pipelines for continuous retraining and updating of a model based on newly received data.\n",
      "\n",
      "## Deploy and test a model\n",
      "\n",
      "## Serving models\n",
      "\n",
      "Deploy your ML models on your OpenShift cluster to test and then integrate them into intelligent applications. When you deploy a model, it is available as a service that you can access by using API calls. You can return predictions based on data inputs that you provide through API calls.\n",
      "\n",
      "## Monitor and manage models\n",
      "\n",
      "## Serving models\n",
      "\n",
      "The Red Hat OpenShift AI service includes model deployment options for hosting the model on Red Hat OpenShift Dedicated or Red Hat Openshift Service on AWS for integration into an external application.\n",
      "\n",
      "## Add accelerators to optimize performance\n",
      "\n",
      "## Working with accelerators\n",
      "\n",
      "If you work with large data sets, you can use accelerators, such as NVIDIA GPUs and Intel Gaudi AI accelerators, to optimize the performance of your data science models in OpenShift AI. With accelerators, you can scale your work, reduce latency, and increase productivity.\n",
      "\n",
      "## Implement distributed workloads for higher performance\n",
      "\n",
      "## Working with distributed workloads\n",
      "\n",
      "Implement distributed workloads to use multiple cluster nodes in parallel for faster, more efficient data processing and model training.\n",
      "\n",
      "## Explore extensions\n",
      "\n",
      "## Working with connected applications\n",
      "\n",
      "Extend your core OpenShift AI solution with integrated third-party applications. Several leading AI/ML software technology partners, including Starburst, Intel AI Tools, Anaconda, and IBM are also available through Red Hat Marketplace.\n",
      "\n",
      "## 5.1. ADDITIONAL RESOURCES\n",
      "\n",
      "In addition to product documentation, Red Hat provides a rich set of learning resources for OpenShift AI and supported applications.\n",
      "\n",
      "On the Resources page of the OpenShift AI dashboard, you can use the category links to filter the resources for various stages of your data science workflow. For example, click the Model serving category to display resources that describe various methods of deploying models. Click All items to show the resources for all categories.\n",
      "\n",
      "For the selected category, you can apply additional options to filter the available resources. For example, you can filter by type, such as how-to articles, quick starts, or tutorials; these resources provide the answers to common questions.\n",
      "\n",
      "For information about Red Hat OpenShift AI support requirements and limitations, see Red Hat OpenShift AI: Supported Configurations.\n"
     ]
    }
   ],
   "source": [
    "## Define Converter\n",
    "converter = DocumentConverter()\n",
    "\n",
    "## Parse document from source and store in variable \"document\"\n",
    "document = converter.convert(source_file)\n",
    "\n",
    "## Print to check everything is working\n",
    "print(document.document.export_to_markdown())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc64833-10bd-4eb9-a769-754735d87695",
   "metadata": {},
   "source": [
    "## 1) Chunking the downloaded document using Docling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7a21665-166f-476c-b91d-59d960dc140e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (631 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learn how to work in an OpenShift AI environment\n",
      "Last Updated: 2024-12-11\n"
     ]
    }
   ],
   "source": [
    "## Parse document from source and store in variable \"document\"\n",
    "converted_source_file = DocumentConverter().convert(source_file)\n",
    "document = converted_source_file.document\n",
    "\n",
    "## Create chubker and chuck document\n",
    "chunker = HybridChunker(tokenizer=\"BAAI/bge-small-en-v1.5\")  # set tokenizer as needed\n",
    "chunk_iter = chunker.chunk(document)\n",
    "\n",
    "## Create chunk_list with the parts of the document\n",
    "chunk_list = list(chunk_iter)\n",
    "\n",
    "#for i,chunk in enumerate(chunk_iter):\n",
    "    #print(i)\n",
    "    #print(chunk)\n",
    "\n",
    "## Print to check everything is working\n",
    "# print(chunk_list[0])\n",
    "print(chunk_list[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2cd433-ae4f-4976-9f8e-751295941edf",
   "metadata": {},
   "source": [
    "## 2) Create embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa57f12d-eba4-4c8f-90c0-fdf8972788a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the Vector: 47\n"
     ]
    }
   ],
   "source": [
    "## Defined embedding model and import it\n",
    "embedding_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "## Define Empty Vector Array\n",
    "vectors = []\n",
    "## Populate Vector Array\n",
    "for i, chunk in enumerate(chunk_list):\n",
    "    vectors.append({\n",
    "        \"id\": i, \n",
    "        \"vector\": embedding_model.get_text_embedding(chunk.text) , \n",
    "        \"text\": chunk.text\n",
    "    })\n",
    "\n",
    "## Print to check everything is working\n",
    "print(f'Length of the Vector: {len(vectors)}' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081006d5-5816-45ec-8dd3-ca4a617094ac",
   "metadata": {},
   "source": [
    "## Upload embeddings to Milvus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fa4fb92-b64d-464c-a33d-072367f5187d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import connections, Collection, FieldSchema, CollectionSchema, DataType\n",
    "\n",
    "## Connect to Milvus (adjust host/port as needed)\n",
    "connections.connect(\"default\", \n",
    "    host=\"vectordb-milvus.milvus.svc.cluster.local\", \n",
    "    port=\"19530\",\n",
    "    token=\"root:Milvus\"\n",
    ")\n",
    "\n",
    "## Define a collection schema (adjust dimensions based on your embedding size)\n",
    "fields = [\n",
    "    FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=False),\n",
    "    FieldSchema(name=\"vector\", dtype=DataType.FLOAT_VECTOR, dim=len(vectors[0][\"vector\"])),\n",
    "    FieldSchema(name=\"text\", dtype=DataType.VARCHAR, max_length=65000),\n",
    "    ## TODO: Add version\n",
    "    #    FieldSchema(name=\"version\", dtype=DataType.VARCHAR, max_length=1024),\n",
    "]\n",
    "schema = CollectionSchema(fields, description=\"RAG embeddings collection\")\n",
    "\n",
    "## TODO: search\n",
    "# res = client.search(\n",
    "#     collection_name=\"my_collection\",\n",
    "#     data=[query_vector],\n",
    "#     limit=5,\n",
    "#     # highlight-start\n",
    "#     filter='color like \"red%\" and likes > 50',\n",
    "#     output_fields=[\"color\", \"likes\"]\n",
    "#     # highlight-end\n",
    "# )\n",
    "\n",
    "\n",
    "## Create or load a collection\n",
    "collection_name = \"rag_embeddings\"\n",
    "collection = Collection(name=collection_name, schema=schema)\n",
    "\n",
    "## TODO: Create an index for faster similarity search\n",
    "index_params = {\"index_type\": \"IVF_FLAT\", \"metric_type\": \"L2\", \"params\": {\"nlist\": len(vectors[0][\"vector\"])}}\n",
    "collection.create_index(field_name=\"vector\", index_params=index_params)\n",
    "\n",
    "## Load the collection for querying\n",
    "collection.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a285316-edb3-495b-be97-9ee02d218f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(insert count: 47, delete count: 0, upsert count: 0, timestamp: 455311954974343175, success count: 47, err count: 0\n"
     ]
    }
   ],
   "source": [
    "## Insert vectors into Milvus\n",
    "insert_output = collection.insert(vectors);\n",
    "\n",
    "## Print to check everything is working\n",
    "print(insert_output)\n",
    "\n",
    "## Load the collection for querying\n",
    "collection.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bc3a4f-464d-4204-9d91-37655d580259",
   "metadata": {},
   "source": [
    "## 3) Query the Database to obtain RAG parsed information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b832f0f-9dd1-4543-be58-fe91395c479c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87120d0-5629-4a72-b371-1db8bf32b42a",
   "metadata": {},
   "source": [
    "## 4) Query Mistral Using RAG information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9c976f4-bdc0-4034-bac0-fdf036d47658",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'markdown_dummy_2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m prompt \u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124mYou are a commentator. Your task is to write a report on an essay.\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124mWhen presented with the essay, come up with interesting questions to ask, and answer each question.\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124mAfterward, combine all the information and write a report in the markdown format.\u001b[39m\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m \u001b[38;5;124m# Essay:\u001b[39m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;132;01m{\u001b[39;00m\u001b[43mmarkdown_dummy_2\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m \u001b[38;5;124m# Instructions:\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124m## Summarize:\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124mIn clear and concise language, summarize the key points and themes presented in the essay.\u001b[39m\n\u001b[1;32m     12\u001b[0m \n\u001b[1;32m     13\u001b[0m \u001b[38;5;124m## Interesting Questions:\u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124mGenerate three distinct and thought-provoking questions that can be asked about the content of the essay. For each question:\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124m- After \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQ: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, describe the problem\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124m- After \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, provide a detailed explanation of the problem addressed in the question.\u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124m- Enclose the ultimate answer in <>.\u001b[39m\n\u001b[1;32m     18\u001b[0m \n\u001b[1;32m     19\u001b[0m \u001b[38;5;124m## Write a report\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124mUsing the essay summary and the answers to the interesting questions, create a comprehensive report in Markdown format.\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'markdown_dummy_2' is not defined"
     ]
    }
   ],
   "source": [
    "prompt =f\"\"\"\n",
    "You are a commentator. Your task is to write a report on an essay.\n",
    "When presented with the essay, come up with interesting questions to ask, and answer each question.\n",
    "Afterward, combine all the information and write a report in the markdown format.\n",
    "\n",
    "# Essay:\n",
    "{markdown_dummy_2}\n",
    "\n",
    "# Instructions:\n",
    "## Summarize:\n",
    "In clear and concise language, summarize the key points and themes presented in the essay.\n",
    "\n",
    "## Interesting Questions:\n",
    "Generate three distinct and thought-provoking questions that can be asked about the content of the essay. For each question:\n",
    "- After \"Q: \", describe the problem\n",
    "- After \"A: \", provide a detailed explanation of the problem addressed in the question.\n",
    "- Enclose the ultimate answer in <>.\n",
    "\n",
    "## Write a report\n",
    "Using the essay summary and the answers to the interesting questions, create a comprehensive report in Markdown format.\n",
    "\"\"\"\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
